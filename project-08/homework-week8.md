# Homework-week8
## 写一个对项目一的整体认知。

从最基础的rnn, lstm开始做sequence to sequence。之后最重要的是深刻理解attention和self attention机制以及encoder和decoder结构。transfomer将attention发扬光大。才有了后来的大规模预训练模型，如bert, gpt-2等。
对于生成任务，有一些重要的问题，如exposure bias，oov，repetition，有相应的tricks和特殊的网络结构，如pgn。对于inference阶段又有一些提升技巧，如beam search。  
对于摘要任务，有bertsum, matchsum.  
对于模型部署，有一些优化技术，如知识蒸馏，剪枝，量化等。











